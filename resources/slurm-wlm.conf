###############################################################################
#                 Sample configuration file for SLURM 21.08
###############################################################################
#
# This file holds the system-wide SLURM configuration. It is read
# by SLURM clients, daemons, and the SLURM API to determine where
# and how to contact the SLURM controller, what other nodes reside
# in the current cluster, and various other configuration information.
#
# SLURM configuration parameters take the form Keyword=Value, where
# at this time, no spacing is allowed to surround the equals (=) sign.
# Many of the config values are not mandatory, and so may be left
# out of the config file.
#
# This simple configuration provides a control machine named "workstation"
# to run the Slurm's central management daemon and a single node
# named "server" which execute jobs. Both machine should have Slurm
# installed and use this configuration file. If you have a similar
# configuration just change the values of SlurmctldHost, for the
# control machine and PartitionName and NodeName for job execution
#
###############################################################################

# ClusterName ##################################################################
#
# The name by which this Slurm managed cluster is known in the
# accounting database. This is needed distinguish accounting records
# when multiple clusters report to the same database. Because of limitations
# in some databases, any upper case letters in the name will be silently mapped
# to lower case. In order to avoid confusion, it is recommended that the name
# be lower case.

ClusterName=cluster

# SlurmctldHost ################################################################
#
# The short, or long, hostname of the machine where Slurm control daemon is
# executed (i.e. the name returned by the command "hostname -s").
# This hostname is optionally followed by the address, either the IP address or
# a name by which the address can be identified, enclosed in parentheses (e.g.
# SlurmctldHost=slurmctl-primary(12.34.56.78)). This value must be specified at
# least once. If specified more than once, the first hostname named will be
# where the daemon runs.
# If the first specified host fails, the daemon will execute on the second host.
# If both the first and second specified host fails, the daemon will execute on
# the third host.

SlurmctldHost=workstation

# ProctrackType ################################################################
#
# Identifies the plugin to be used for process tracking on a job step basis.
# The slurmd daemon uses this mechanism to identify all processes
# which are children of processes it spawns for a user job step.
# The slurmd daemon must be restarted for a change in ProctrackType
# to take effect.
#
# NOTE: "proctrack/linuxproc" and "proctrack/pgid" can fail to
# identify all processes associated with a job since processes
# can become a child of the init process (when the parent process
# terminates) or change their process group.
# To reliably track all processes, "proctrack/cgroup" is highly recommended.
#
# NOTE: The JobContainerType applies to a job allocation, while
# ProctrackType applies to job steps.
#
# Acceptable values at present include:
#
# proctrack/cgroup      Uses linux cgroups to constrain and track processes,
#                       and is the default for systems with cgroup support.
#                       NOTE: see "man cgroup.conf" for configuration details.
#
# proctrack/cray_aries  Uses Cray proprietary process tracking.
#
# proctrack/linuxproc   Uses linux process tree using parent process IDs.
#
# proctrack/pgid        Uses Process Group IDs.
#                       NOTE: This is the default for the BSD family.

ProctrackType=proctrack/linuxproc

# ReturnToService ##############################################################
#
# Controls when a DOWN node will be returned to service.
# The default value is 0.
# Supported values include
#
# 0  A node will remain in the DOWN state until a system administrator
#    explicitly changes its state (even if the slurmd daemon registers and
#    resumes communications).
#
# 1  A DOWN node will become available for use upon registration with a valid
#    configuration only if it was set DOWN due to being non-responsive.  If the
#    node was set DOWN for any other reason (low memory, unexpected reboot,
#    etc.), its state will not automatically be changed. A node registers with
#    a valid configuration if its memory, GRES, CPU count, etc. are equal to or
#    greater than the values configured in slurm.conf.
#
# 2  A DOWN node will become available for use upon registration with a valid
#    configuration. The node could have been set DOWN for any reason. A node
#    registers with a valid configuration if its memory, GRES, CPU count, etc.
#    are equal to or greater than the values configured in slurm.conf.

ReturnToService=2

# SlurmctldPidFile #############################################################
#
# Fully qualified pathname of a file into which the  slurmctld daemon
# may write its process id. This may be used for automated signal processing.
# This parametes should be set to /run/slurmctld.pid on Debian systems

SlurmctldPidFile=/run/slurmctld.pid

# SlurmdPidFile ################################################################
#
# Fully qualified pathname of a file into which the slurmd daemon may write
# its process id. This may be used for automated signal processing.
# Any "%h" within the name is replaced with the hostname on which the
# slurmd is running.
# Any "%n" within the name is replaced with the Slurm node name on which the
# slurmd is running.
# This parametes should be set to /run/slurmd.pid on Debian systems

SlurmdPidFile=/run/slurmd.pid

# SlurmdSpoolDir ###############################################################
#
# Fully qualified pathname of a directory into which the slurmd
# daemon's state information and batch job script information are written. This
# must be a common pathname for all nodes, but should represent a directory which
# is local to each node (reference a local file system).
# This parametes should be set to /var/lib/slurm/slurmd on Debian systems
# Any "%h" within the name is replaced with the hostname on which the
# slurmd is running.
# Any "%n" within the name is replaced with the Slurm node name on which the
# slurmd is running.

SlurmdSpoolDir=/var/lib/slurm/slurmd

# StateSaveLocation ############################################################
#
# Fully qualified pathname of a directory into which the Slurm controller,
# slurmctld, saves its state (e.g. "/usr/local/slurm/checkpoint").
# Slurm state will saved here to recover from system failures.
# SlurmUser must be able to create files in this directory.
# If you have a secondary SlurmctldHost configured, this location should be
# readable and writable by both systems.
# Since all running and pending job information is stored here, the use of
# a reliable file system (e.g. RAID) is recommended.
# This parametes should be set to /var/lib/slurm/slurmctld on Debian systems
# If any slurm daemons terminate abnormally, their core files will also be
# written into this directory.

StateSaveLocation=/var/lib/slurm/slurmctld

# SlurmUser ####################################################################
#
# The name of the user that the slurmctld daemon executes as.
# For security purposes, a user other than "root" is recommended.
# This user must exist on all nodes of the cluster for authentication
# of communications between Slurm components.
# The default value is "root".

SlurmUser=slurm

# TaskPlugin ###################################################################
#
# Identifies the type of task launch plugin, typically used to provide
# resource management within a node (e.g. pinning tasks to specific
# processors). More than one task plugin can be specified in a comma-separated
# list. The prefix of "task/" is optional. Acceptable values include:
#
# task/affinity  enables resource containment using sched_setaffinity().  This
#                enables the --cpu-bind and/or --mem-bind srun options.
# task/cgroup    enables resource containment using Linux control cgroups.
#                This enables the --cpu-bind and/or --mem-bind srun options.
#                NOTE: see "man cgroup.conf" for configuration details.
# task/none      for systems requiring no special handling of user tasks.
#                Lacks support for the --cpu-bind and/or --mem-bind srun
#                options. The default value is "task/none".

TaskPlugin=task/none

# SchedulerType ################################################################
#
# Identifies the type of scheduler to be used.
# Note the slurmctld daemon must be restarted for a change in
# scheduler type to become effective (reconfiguring a running daemon has
# no effect for this parameter).
# The scontrol command can be used to manually change job priorities
# if desired.
# Acceptable values include:
#
# sched/backfill  For a backfill scheduling module to augment the default FIFO
#                 scheduling.  Backfill scheduling will initiate lower-priority
#                 jobs if doing so does not delay the expected initiation time
#                 of any higher priority job.  Effectiveness of backfill
#                 scheduling is dependent upon users specifying job time limits,
#                 otherwise all jobs will have the same time limit and
#                 backfilling is impossible.
#
# sched/builtin   This is the FIFO scheduler which initiates jobs in priority
#                 order.  If any job in the partition can not be scheduled, no
#                 lower priority job in that partition will be scheduled.
#                 An exception is made for jobs that can not run due to
#                 partition constraints (e.g. the time limit) or down/drained
#                 nodes. In that case, lower priority jobs can be initiated and
#                 not impact the higher priority job.

SchedulerType=sched/backfill

# SelectType ###################################################################
#
# Identifies the type of resource selection algorithm to be used.
# Changing this value can only be done by restarting the slurmctld daemon.
# When changed, all job information (running and pending) will be lost,
# since the job state save format used by each plugin is different.
# The only exception to this is when changing from cons_res to cons_tres or from
# cons_tres to cons_res. However, if a job contains cons_tres-specific features
# and then SelectType is changed to cons_res, the job will be canceled, since
# there is no way for cons_res to satisfy requirements specific to cons_tres.
# Acceptable values include
#
# select/cons_res    The resources (cores and memory) within a node are
#                    individually allocated as consumable resources. Note that
#                    whole nodes can be allocated to jobs for selected
#                    partitions by using the OverSubscribe=Exclusive option.
#                    See the partition OverSubscribe parameter for more
#                    information.
#
# select/cons_tres   The resources (cores, memory, GPUs and all other trackable
#                    resources) within a node are individually allocated as
#                    consumable resources. Note that whole nodes can be
#                    allocated to jobs for selected partitions by using the
#                    OverSubscribe=Exclusive option. See the partition
#                    OverSubscribe parameter for more information.
#
# select/cray_aries  for a Cray system. The default value is
#                    "select/cray_aries" for all Cray systems.
#
# select/linear      for allocation of entire nodes assuming a one-dimensional
#                    array of nodes in which sequentially ordered nodes are
#                    preferable. For a heterogeneous cluster (e.g. different
#                    CPU counts on the various nodes), resource allocations
#                    will favor nodes with high CPU counts as needed based
#                    upon the job's node and CPU specification if
#                    TopologyPlugin=topology/none is configured. Use of other
#                    topology plugins with select/linear and heterogeneous
#                    nodes is not recommended and may result in valid job
#                    allocation requests being rejected. This is the default
#                    value.

SelectType=select/cons_tres

# SelectTypeParameters #########################################################
#
# Partition-specific resource allocation type.
# This option replaces the global SelectTypeParameters value.
# Supported values are CR_Core, CR_Core_Memory, CR_Socket and
# CR_Socket_Memory.
# Use requires the system-wide SelectTypeParameters value be set to
# any of the four supported values previously listed; otherwise, the
# partition-specific value will be ignored.

SelectTypeParameters=CR_Core_Memory

# AccountingStorageType ########################################################
#
# The accounting storage mechanism type.  Acceptable values at
# present include "accounting_storage/none"
# and "accounting_storage/slurmdbd".
# The "accounting_storage/slurmdbd" value indicates that accounting records
# will be written to the Slurm DBD, which manages an underlying MySQL
# database. See "man slurmdbd" for more information.  The
# default value is "accounting_storage/none" and indicates that account
# records are not maintained.

AccountingStorageType=accounting_storage/none

# JobCompType ##################################################################
#
# The job completion logging mechanism type.
# Acceptable values at present include:
#
# jobcomp/none           Upon job completion, a record of the job is purged
#                        from the system. If using the accounting
#                        infrastructure this plugin may not be of interest
#                        since some of the information is redundant.
# jobcomp/elasticsearch  Upon job completion, a record of the job should be
#                        written to an Elasticsearch server, specified by the
#                        JobCompLoc parameter.
#                        NOTE: More information is available at the Slurm web
#                        site (https://slurm.schedmd.com/elasticsearch.html).
# jobcomp/filetxt        Upon job completion, a record of the job should be
#                        written to a text file, specified by the JobCompLoc
#                        parameter.
# jobcomp/lua            Upon job completion, a record of the job should be
#                        processed by the jobcomp.lua script, located in the
#                        default script directory (typically the subdirectory
#                        etc of the installation directory.
# jobcomp/mysql          Upon job completion, a record of the job should be
#                        written to a MySQL or MariaDB database, specified by
#                        the JobCompLoc parameter.
# jobcomp/script         Upon job completion, a script specified by the
#                        JobCompLoc parameter is to be executed with
#                        environment variables providing the job information.

JobCompType=jobcomp/none

# JobAcctGatherType ############################################################
#
# The job accounting mechanism type.
# Acceptable values at present include "jobacct_gather/linux" (for Linux
# systems) and is the recommended one, "jobacct_gather/cgroup" and
# "jobacct_gather/none" (no accounting data collected).
# The default value is "jobacct_gather/none".
# "jobacct_gather/cgroup" is a plugin for the Linux operating system
# that uses cgroups to collect accounting statistics. The plugin collects the
# following statistics: From the cgroup memory subsystem: memory.usage_in_bytes
# (reported as 'pages') and rss from memory.stat (reported as 'rss'). From the
# cgroup cpuacct subsystem: user cpu time and system cpu time. No value
# is provided by cgroups for virtual memory size ('vsize').
# In order to use the sstat tool "jobacct_gather/linux",
# or "jobacct_gather/cgroup" must be configured.
#
# NOTE: Changing this configuration parameter changes the contents of
# the messages between Slurm daemons. Any previously running job steps are
# managed by a slurmstepd daemon that will persist through the lifetime of
# that job step and not change its communication protocol. Only change this
# configuration parameter when there are no running job steps.

JobAcctGatherType=jobacct_gather/none

# SlurmctldDebug ###############################################################
#
# The level of detail to provide slurmctld daemon's logs.
# The default value is info.
# If the slurmctld daemon is initiated with -v or --verbose options,
# that debug level will be preserve or restored upon reconfiguration.
#
# quiet    Log nothing
# fatal    Log only fatal errors
# error    Log only errors
# info     Log errors and general informational messages
# verbose  Log errors and verbose informational messages
# debug    Log errors and verbose informational messages and debugging messages
# debug2   Log errors and verbose informational messages and more debugging
#          messages
# debug3   Log errors and verbose informational messages and even more
#          debugging messages

SlurmctldDebug=info

# SlurmctldLogFile #############################################################
#
# Fully qualified pathname of a file into which the slurmctld daemon's
# logs are written.
# The default value is none (performs logging via syslog).

SlurmctldLogFile=/var/log/slurm/slurmctld.log

# SlurmdDebug ##################################################################
# The level of detail to provide slurmd daemon's logs.
# The default value is info.
#
# quiet    Log nothing
# fatal    Log only fatal errors
# error    Log only errors
# info     Log errors and general informational messages
# verbose  Log errors and verbose informational messages
# debug    Log errors and verbose informational messages and debugging messages
# debug2   Log errors and verbose informational messages and more debugging
#          messages
# debug3   Log errors and verbose informational messages and even more
#          debugging messages

SlurmdDebug=info

# SlurmdLogFile ################################################################
#
# Fully qualified pathname of a file into which the  slurmd daemon's
# logs are written.
# The default value is none (performs logging via syslog).
# Any "%h" within the name is replaced with the hostname on which the
# slurmd is running.
# Any "%n" within the name is replaced with the Slurm node name on which the
# slurmd is running.

SlurmdLogFile=/var/log/slurm/slurmd.log

# NodeName #####################################################################
#
# Name that Slurm uses to refer to a node.  Typically this would be the string
# that "/bin/hostname -s" returns.  It may also be the fully qualified domain
# name as returned by "/bin/hostname -f" (e.g. "foo1.bar.com"), or any valid
# domain name associated with the host through the host database (/etc/hosts)
# or DNS, depending on the resolver settings.  Note that if the short form of
# the hostname is not used, it may prevent use of hostlist expressions (the
# numeric portion in brackets must be at the end of the string).  It may also
# be an arbitrary string if NodeHostname is specified.  If the NodeName is
# "DEFAULT", the values specified with that record will apply to subsequent
# node specifications unless explicitly set to other values in that node record
# or replaced with a different set of default values.  Each line where NodeName
# is "DEFAULT" will replace or add to previous default values and not a
# reinitialize the default values.  For architectures in which the node order
# is significant, nodes will be considered consecutive in the order defined.
# For example, if the configuration for "NodeName=charlie" immediately follows
# the configuration for "NodeName=baker" they will be considered adjacent in
# the computer.
# See slurm.conf(5) man page for details
# In the following configuration we specify a host called server with 16 cores
# and 64GB of memory

NodeName=server CPUs=16 RealMemory=64000

# PartitionName ################################################################
#
# Name by which the partition may be referenced (e.g. "Interactive").  This
# name can be specified by users when submitting jobs. If the PartitionName is
# "DEFAULT", the values specified with that record will apply to subsequent
# partition specifications unless explicitly set to other values in that
# partition record or replaced with a different set of default values. Each
# line where PartitionName is "DEFAULT" will replace or add to previous default
# values and not a reinitialize the default values.

PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP
